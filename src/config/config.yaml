# ======================================================
# HYDRA CONFIGURATION
# ======================================================
defaults:
  - dataset: cifar100
  - model: vit_base_patch16_224.orig_in21k
  - optimizer: adam
  - scheduler: cosine
  - _self_

# ======================================================
# EXPERIMENT SETTINGS
# ======================================================
# Experiment identification and naming
experiment:
  name: "${dataset.name}_${model.name}_${continual.strategy}_${now:%Y-%m-%d_%H-%M-%S-%f}"

# ======================================================
# SYSTEM SETTINGS
# ======================================================
# Basic system configuration
seed: 42
device: cuda
num_workers: 4
eval_only: false

# Distributed training configuration
distributed:
  enabled: false  # Set to true to enable distributed training
  world_size: ${oc.env:WORLD_SIZE,${oc.decode:${oc.env:CUDA_VISIBLE_DEVICES,0}:length}}  # Number of GPUs to use

# ======================================================
# TRAINING CONFIGURATION
# ======================================================
# Continual learning parameters
continual:
  num_steps: 10                # Number of continual learning steps
  classes_per_step: 10         # Number of classes per step
  first_step_classes: null     # Number of classes in first step (null = use classes_per_step)
  memory_size: 2000            # Size of memory buffer for rehearsal (if used)
  strategy: "finetune"         # Options: finetune, ewc, replay, zeroshot, prompt_tuning, ema etc.
  mask_logits: true            # Enable logit masking for non-current task classes, except replay samples
  entropy_prediction: false    # Enable entropy-based logit selection before prediction
  
  # Prototypical classifier parameters
  prototypical:
    init_with_pretrained: false  # Use pretrained model for prototype initialization instead of current step model
    replace_classifiers: false   # Replace classifiers with new prototypes after each step

  # EMA parameters (used when strategy is "ema")
  ema:
    momentum: 0.999            # EMA momentum (higher = slower update)
    eval_with_teacher: false   # Use teacher model for final evaluation instead of student
    refresh_interval: null     # Refresh teacher every N epochs (null = refresh only at task start)
    refresh_at_step_start: true # Refresh teacher at start of each step (except relies on refresh_interval)
    skip_names: ["classifier", "head", "fc"]  # Parameter names to skip during EMA updates
    momentum_overrides: {}     # Override momentum for specific parameter names (e.g., {"classifier": 0.9})

  # Classifier calibration parameters
  calibration:
    enabled: false             # Enable classifier calibration for continual learning
    classifier_as_prototype: true # Use classifier as prototype for calibration
    eval_with_calibration: true # Apply calibration during training evaluation (temporary)
    method: "translation"       # Calibration method: "translation", "rigid", "affine", or "nonlinear"
    regularization_weight: 0.01 # Regularization weight for nonlinear transform (L2 penalty)
    strength: 1.0              # Calibration strength: 0.0 = original prototypes only, 1.0 = calibrated prototypes only

  # Competitive distillation parameters (for hybrid mode with text + learned classifier)
  distillation:
    enabled: false             # Enable competitive bidirectional distillation
    gt_loss_weight: 1.0        # Weight for ground truth supervision loss
    distill_loss_weight: 0.5   # Weight for distillation loss
    temperature: 2.0           # Temperature for distillation (higher = softer targets)
    loss_ratio_clip: 5.0       # Clip loss ratio to avoid extreme values
    epsilon: 1e-8              # Small constant to avoid division by zero

  # Prompt tuning parameters (used when strategy is "prompt_tuning")
  prompt_tuning:
    prompt_length: 10            # Number of prompt tokens
    init_type: "random"          # Options: random, uniform, xavier
    prompt_dropout: 0.0          # Dropout rate for prompt tokens
    prompt_pool: false           # Use a pool of prompts to select from
    pool_size: 10                # Size of prompt pool (if prompt_pool is true)
    key_diversity_regularization: false  # Add diversity regularization for prompt selection
    frequency_diversity_regularization: false  # Add L2P-style frequency-based diversity regularization
    top_k: 4                     # Number of top prompts to select from pool
    similarity_weight: 0.01      # Weight for similarity loss between queries and keys
    diversity_weight: 0.01       # Weight for diversity regularization loss
    omit_cls_token: false        # Whether to omit CLS token when inserting prompts, use avg pooled prompt instead
    init_config:                 # Initialization configuration
      normal_std: 0.02           # Standard deviation for normal initialization
      uniform_range: [-0.1, 0.1] # Range for uniform initialization [min, max]

  # LoRA (Low-Rank Adaptation) parameters (used when strategy is "lora")
  lora:
    rank: 8                      # Rank of LoRA matrices (r in the paper)
    alpha: 16                    # LoRA scaling factor (alpha / r is the final scaling)
    dropout: 0.1                 # Dropout rate for LoRA layers
    target_modules:              # List of module name patterns to apply LoRA to
      # --- For timm ViT models (e.g., vit_base_patch16_224) ---
      - "qkv"                    # Query, Key, Value projections in attention
      - "proj"                   # Output projection in attention
      - "fc1"                    # First linear layer in MLP
      - "fc2"                    # Second linear layer in MLP
      # --- For OpenCLIP/CLIP models (ResidualAttentionBlock) ---
      # With MultiheadAttention support, you can now adapt all attention projections:
      - "attn"                   # Adapt Q, K, V, out_proj (see multihead_attention config below)
      # - "attn.out_proj"        # Alternative: Only adapt output projection (old behavior)
      - "mlp.c_fc"               # MLP first layer
      - "mlp.c_proj"             # MLP second layer
      # --- For other architectures ---
      # - "attention.query"      # Separate Q projection
      # - "attention.key"        # Separate K projection  
      # - "attention.value"      # Separate V projection
    merge_weights: false         # Merge LoRA weights into original weights after training
    bias: "none"                 # Bias handling: "none", "all", or "lora_only"
    fan_in_fan_out: false        # Set to true for Conv1D layers (e.g., GPT-2 style)
    # Fine-grained control for nn.MultiheadAttention adaptation
    multihead_attention:
      adapt_q: true              # Adapt Query projection
      adapt_k: true              # Adapt Key projection
      adapt_v: true              # Adapt Value projection
      adapt_out: true            # Adapt output projection
      # Example: For LoRA paper's recommendation, set adapt_k: false

  # Adapter tuning parameters (used when strategy is "adapter")
  adapter:
    bottleneck_dim: 64           # Dimension of adapter bottleneck (lower = more efficient)
    dropout: 0.1                 # Dropout rate for adapter layers
    init_scale: 1e-3             # Scale for adapter weight initialization
    activation: "gelu"           # Activation function: "gelu", "relu", "silu"
    # For CLIP models with separate vision and text encoders
    adapt_vision: true           # Apply adapters to vision encoder
    adapt_text: true             # Apply adapters to text encoder
    # Adapter placement (after which components)
    adapter_after_attn: false    # Insert adapter after attention (before MLP)
    adapter_after_mlp: true      # Insert adapter after MLP (standard placement)

  # Projection tuning parameters (used when strategy is "projection")
  projection:
    projection_type: "linear"    # Type of projection: "linear", "mlp", "residual"
    hidden_dim: null             # Hidden dimension for MLP projection (null = auto, same as input)
    dropout: 0.1                 # Dropout rate for projection layers and PROOF fusion (default: 0.1)
    activation: "gelu"           # Activation function for MLP: "gelu", "relu", "silu"
    num_layers: 1                # Number of layers for MLP projection (1 = linear)
    init_scale: 0.01             # Scale for weight initialization
    # For CLIP models with separate vision and text encoders
    adapt_vision: true           # Apply projection to vision encoder (backbone)
    adapt_text: true             # Apply projection to text encoder
    # Expandable projection for continual learning (inspired by PROOF)
    expandable: false            # Learn new projection per step and fuse with previous ones
    fusion_method: "add"         # Fusion method: "add", "weighted_sum", "attention", "gated"
    learnable_fusion: false      # Whether fusion weights/gates are learnable
    freeze_previous: true        # Freeze previous projections when learning new ones
    # PROOF fusion layer (combines image/text features with prototypes and context prompts)
    use_proof_fusion: false      # Enable PROOF-style self-attention fusion layer
    num_context_prompts: 3       # Number of learnable context prompts per task

# Training hyperparameters
training:
  num_epochs: 100              # Maximum epochs per step
  batch_size: 64               # Batch size for training
  eval_batch_size: 64          # Batch size for evaluation
  eval_every: 1                # Evaluate every N epochs
  save_every: 10               # Save checkpoint every N epochs
  early_stopping_patience: 10  # Patience for early stopping
  gradient_clipping:
    enabled: true              # Enable gradient clipping
    max_norm: 1.0              # Maximum norm of gradients
  mixed_precision:
    enabled: true              # Enable mixed precision training
    dtype: "auto"              # Options: auto, bfloat16, float16 (auto will use bfloat16 if supported, otherwise float16)
    eval: true                 # Enable mixed precision during evaluation

# ======================================================
# OUTPUT AND LOGGING
# ======================================================
# Logging configuration
logging:
  tensorboard: true            # Enable TensorBoard logging
  wandb: true                  # Enable Weights & Biases logging
  wandb_project: "continual-learning"
  wandb_entity: null           # Set to your wandb username or team name
  log_grad_norms: false        # Log gradient norms for each parameter layer

# Path configuration
paths:
  # Input paths
  data_dir: "${hydra:runtime.cwd}/data"            # Data directory
  cache_dir: "${hydra:runtime.cwd}/cache"          # Cache directory for model weights
  
  # Output paths
  output_root: "${hydra:runtime.cwd}/outputs"      # Root directory for all outputs
  log_dir: "${paths.output_root}/logs"     # Directory for log files
  plots_dir: "${paths.output_root}/plots"  # Directory for plot files
  checkpoint_dir: "${paths.output_root}/checkpoints"  # Directory for model checkpoints
  wandb_dir: "${paths.output_root}/wandb"  # Store wandb files in outputs directory

# ======================================================
# DEBUGGING TOOLS
# ======================================================
debug:
  enabled: false               # Set to true to enable debug mode
  verbose: true                # Enable verbose logging
  fast_dev_run: false          # Run only a few batches for quick testing
  limit_train_batches: 1.0     # Fraction of training batches to use (1.0 = all)
  limit_val_batches: 1.0       # Fraction of validation batches to use (1.0 = all)
  log_every_n_steps: 1         # Log metrics every N steps

# ======================================================
# HYDRA RUNTIME SETTINGS
# ======================================================
hydra:
  run:
    dir: ${paths.output_root}/runs/${experiment.name}
  sweep:
    dir: ${paths.output_root}/multirun/${experiment.name}
  job_logging:
    handlers:
      file:
        filename: ${hydra.run.dir}/hydra.log
