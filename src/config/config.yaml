# ======================================================
# HYDRA CONFIGURATION
# ======================================================
defaults:
  - dataset: cifar100
  - model: vit_base_patch16_224.orig_in21k
  - optimizer: adam
  - scheduler: cosine
  - _self_

# ======================================================
# EXPERIMENT SETTINGS
# ======================================================
# Experiment identification and naming
experiment:
  name: "${dataset.name}_${model.name}_${continual.strategy}_${now:%Y-%m-%d_%H-%M-%S}"

# ======================================================
# SYSTEM SETTINGS
# ======================================================
# Basic system configuration
seed: 42
device: cuda
num_workers: 4
eval_only: false

# Distributed training configuration
distributed:
  enabled: false  # Set to true to enable distributed training
  world_size: ${oc.env:WORLD_SIZE,${oc.decode:${oc.env:CUDA_VISIBLE_DEVICES,0}:length}}  # Number of GPUs to use

# ======================================================
# TRAINING CONFIGURATION
# ======================================================
# Continual learning parameters
continual:
  num_steps: 10                # Number of continual learning steps
  classes_per_step: 10         # Number of classes per step
  memory_size: 2000            # Size of memory buffer for rehearsal (if used)
  strategy: "finetune"         # Options: finetune, ewc, replay, zeroshot, prompt_tuning, ema etc.
  mask_logits: true            # Enable logit masking for non-current task classes, except replay samples
  entropy_prediction: false    # Enable entropy-based logit selection before prediction
  
  # Prototypical classifier parameters
  prototypical:
    init_with_pretrained: false  # Use pretrained model for prototype initialization instead of current step model

  # EMA parameters (used when strategy is "ema")
  ema:
    momentum: 0.999            # EMA momentum (higher = slower update)
    eval_with_teacher: false   # Use teacher model for final evaluation instead of student
    refresh_interval: null     # Refresh teacher every N epochs (null = refresh only at task start)
    refresh_at_step_start: true # Refresh teacher at start of each step (except relies on refresh_interval)
    skip_names: ["classifier", "head", "fc"]  # Parameter names to skip during EMA updates
    momentum_overrides: {}     # Override momentum for specific parameter names (e.g., {"classifier": 0.9})

  # Prompt tuning parameters (used when strategy is "prompt_tuning")
  prompt_tuning:
    prompt_length: 10            # Number of prompt tokens
    init_type: "random"          # Options: random, uniform, xavier
    prompt_dropout: 0.0          # Dropout rate for prompt tokens
    prompt_pool: false           # Use a pool of prompts to select from
    pool_size: 10                # Size of prompt pool (if prompt_pool is true)
    key_diversity_regularization: false  # Add diversity regularization for prompt selection
    frequency_diversity_regularization: false  # Add L2P-style frequency-based diversity regularization
    top_k: 4                     # Number of top prompts to select from pool
    similarity_weight: 0.01      # Weight for similarity loss between queries and keys
    diversity_weight: 0.01       # Weight for diversity regularization loss
    omit_cls_token: false        # Whether to omit CLS token when inserting prompts, use avg pooled prompt instead
    init_config:                 # Initialization configuration
      normal_std: 0.02           # Standard deviation for normal initialization
      uniform_range: [-0.1, 0.1] # Range for uniform initialization [min, max]

# Training hyperparameters
training:
  num_epochs: 100              # Maximum epochs per step
  batch_size: 64               # Batch size for training
  eval_batch_size: 64          # Batch size for evaluation
  eval_every: 1                # Evaluate every N epochs
  save_every: 10               # Save checkpoint every N epochs
  early_stopping_patience: 10  # Patience for early stopping
  gradient_clipping:
    enabled: true              # Enable gradient clipping
    max_norm: 1.0              # Maximum norm of gradients
  mixed_precision:
    enabled: true              # Enable mixed precision training
    dtype: "auto"              # Options: auto, bfloat16, float16 (auto will use bfloat16 if supported, otherwise float16)
    eval: true                 # Enable mixed precision during evaluation

# ======================================================
# OUTPUT AND LOGGING
# ======================================================
# Logging configuration
logging:
  tensorboard: true            # Enable TensorBoard logging
  wandb: true                  # Enable Weights & Biases logging
  wandb_project: "continual-learning"
  wandb_entity: null           # Set to your wandb username or team name
  log_grad_norms: false        # Log gradient norms for each parameter layer

# Path configuration
paths:
  # Input paths
  data_dir: "${hydra:runtime.cwd}/data"            # Data directory
  cache_dir: "${hydra:runtime.cwd}/cache"          # Cache directory for model weights
  
  # Output paths
  output_root: "${hydra:runtime.cwd}/outputs"      # Root directory for all outputs
  log_dir: "${paths.output_root}/logs"     # Directory for log files
  plots_dir: "${paths.output_root}/plots"  # Directory for plot files
  checkpoint_dir: "${paths.output_root}/checkpoints"  # Directory for model checkpoints
  wandb_dir: "${paths.output_root}/wandb"  # Store wandb files in outputs directory

# ======================================================
# DEBUGGING TOOLS
# ======================================================
debug:
  enabled: false               # Set to true to enable debug mode
  verbose: true                # Enable verbose logging
  fast_dev_run: false          # Run only a few batches for quick testing
  limit_train_batches: 1.0     # Fraction of training batches to use (1.0 = all)
  limit_val_batches: 1.0       # Fraction of validation batches to use (1.0 = all)
  log_every_n_steps: 1         # Log metrics every N steps

# ======================================================
# HYDRA RUNTIME SETTINGS
# ======================================================
hydra:
  run:
    dir: ${paths.output_root}/runs/${experiment.name}
  sweep:
    dir: ${paths.output_root}/multirun/${experiment.name}
  job_logging:
    handlers:
      file:
        filename: ${hydra.run.dir}/hydra.log
