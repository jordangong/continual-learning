name: ViT-B-16
source: openclip  # Options: timm, openclip
pretrained: laion400m_e32
freeze_backbone: false  # Whether to freeze the visual backbone during training
freeze_classifier: false  # Whether to freeze the classifier during training
feature_dim: 512  # Feature dimension for ViT-B/16 with projection
skip_blocks: 0  # Number of final transformer blocks to skip
skip_final_norm: false  # Whether to skip final norm layer after transformer blocks
skip_proj: false  # Keep projection to match text embedding dimension

# CLIP text-based classifier configuration
classifier:
  type: clip_text  # Use CLIP text encoder for classification
  mode: text  # Options: text (text encoder only), hybrid (text + learned classifier)

  # Text prompt templates (use {} as placeholder for class name)
  text_templates:
    - "a photo of a {}."
    - "a photo of the {}."

  # Text encoder settings
  freeze_text_encoder: true  # True=zero-shot (frozen), False=train text encoder
  ensemble_text: true  # Average embeddings from multiple templates

  # Similarity settings
  normalize: true  # L2 normalize features (required for cosine similarity)
  temperature: 100.0  # CLIP temperature (only used if use_pretrained_temperature=false)
  use_pretrained_temperature: true  # Use CLIP's pre-trained logit_scale as temperature (recommended)
  learnable_temperature: false  # Make temperature a learnable parameter
  use_log_temperature: false  # If learnable_temperature=true, parameterize in log space
  logit_bias: null  # Manual logit_bias value (only used if use_pretrained_logit_bias=false)
  use_pretrained_logit_bias: false  # Use CLIP's pre-trained logit_bias (only for CustomTextCLIP models)
  learnable_logit_bias: false  # Make logit_bias a learnable parameter

  # Hybrid mode settings (only used if mode: hybrid)
  hybrid_weight: 0.5  # Weight for text vs learned (0.0=learned only, 1.0=text only)
  learnable_hybrid_weight: false  # Make hybrid_weight a learnable parameter

  # Learned classifier settings (for linear/hybrid modes)
  learned_classifier_type: linear  # Options: linear, mlp, prototypical
  hidden_dim: null  # Hidden dimension (required if learned_classifier_type=mlp)
  dropout: 0.0  # Dropout for MLP classifier

  # Loss function settings
  # Default: use_pretraining_loss=false -> standard cross-entropy loss
  # For CLIP/SigLIP pretraining: set use_pretraining_loss=true
  # Hybrid mode: supports pretraining loss but REQUIRES use_regular_loss=true
  use_pretraining_loss: false  # Enable CLIP/SigLIP contrastive loss (within-batch)
  pretraining_loss_type: "clip"  # Options: clip (ClipLoss), siglip (SigLipLoss)
  pretraining_loss_weight: 1.0  # Weight for pretraining loss
  learnable_pretraining_loss_weight: false  # Make pretraining_loss_weight a learnable parameter
  use_regular_loss: false  # When use_pretraining_loss=true, also add standard cross-entropy (REQUIRED for hybrid mode)
  regular_loss_weight: 1.0  # Weight for regular cross-entropy loss when using both
  learnable_regular_loss_weight: false  # Make regular_loss_weight a learnable parameter
  supervised_contrastive: false  # Use supervised contrastive loss (treats same-class samples as positives)

sae:
  use_sae: false
  checkpoint_path: "/path/to/your/sae_checkpoint.pt"
  layer: -2
  token_type: all
